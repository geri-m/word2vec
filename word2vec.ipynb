{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4nZ6bdT0blT30feDvqVcD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geri-m/word2vec/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teduj7O3P2mc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "32c5506b-446e-463f-de59-de3fa1f8b31f"
      },
      "source": [
        "# https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296\n",
        "import multiprocessing\n",
        "\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')\n",
        "\n",
        "# Data is processed and Tokenised!\n",
        "sentences = brown.sents()\n",
        "print(sentences[:3])\n",
        "\n",
        "EMB_DIM = 300\n",
        "\n",
        "w2v = Word2Vec(sentences, size=EMB_DIM, window=5, min_count=5, negative=15, iter=10,\n",
        "               workers=multiprocessing.cpu_count())\n",
        "\n",
        "word_vectors = w2v.wv  # get trained embeddings - an KeyedVector instaces\n",
        "result = word_vectors.similar_by_word(\"Saturday\")\n",
        "print(\"Most Similar to 'Saturday': %s\" % result[:3])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n",
            "Most Similar to 'Saturday': [('Monday', 0.8949118256568909), ('Sunday', 0.8893466591835022), ('Friday', 0.874451756477356)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP28W_vPQjcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import conll2000\n",
        "from gensim.models import Word2Vec # https://code.google.com/archive/p/word2vec/\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "train_words = conll2000.tagged_words('train.txt')\n",
        "test_words = conll2000.tagged_words('test.txt')\n",
        "print(train_words[:20])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDq9g5uyQmG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tag_vocabulary(tagged_words):\n",
        "  tag2id = {}\n",
        "  for item in tagged_words:\n",
        "    tag = item[1]\n",
        "    tag2id.setdefault(tag, len(tag2id))\n",
        "\n",
        "  return tag2id\n",
        "\n",
        "word2id = {k: v.index for k, v in word_vectors.vocab.items()}\n",
        "tag2id = get_tag_vocabulary(train_words) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxe0HHoIQpTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_int_data(tagged_words, word2id, tag2id):\n",
        "  X, Y = [], []\n",
        "  unk_count = 0\n",
        "\n",
        "  for word, tag in tagged_words:\n",
        "    Y.append(tag2id.get(tag))\n",
        "    if word in word2id:\n",
        "      X.append(word2id.get(word))\n",
        "    else:\n",
        "      unk_count += 1\n",
        "\n",
        "\n",
        "    print(\"Data Created. percentag of unkonw words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X_train, Y_train = get_int_data(train_words, word2id, tag2id)\n",
        "X_test, Y_test = get_int_data(test_words, word2id, tag2id)\n",
        "\n",
        "# we need to one-hot encode the tag indexes\n",
        "Y_train, Y_test = to_categorical(Y_train), to_categorical(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}