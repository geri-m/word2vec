{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNN5rxYJOyfq/VQeKWhSYAj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geri-m/word2vec/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiGZUy93WbIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "7a6d2363-6564-4678-e965-58c820208a8a"
      },
      "source": [
        "# The sample requires Tensorflow 1.x and therefore we need to explicitly install it\n",
        "!pip install tensorflow==2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.1 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (2.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.10.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (2.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.12.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.18.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.34.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1) (46.1.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1) (2.10.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2020.4.5.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teduj7O3P2mc",
        "colab_type": "code",
        "outputId": "bd7d60b7-0b27-41f2-a313-986239d40775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296\n",
        "import multiprocessing\n",
        "\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')\n",
        "\n",
        "# Data is processed and Tokenised!\n",
        "sentences = brown.sents()\n",
        "print(sentences[:3])\n",
        "\n",
        "EMB_DIM = 300\n",
        "\n",
        "w2v = Word2Vec(sentences, size=EMB_DIM, window=5, min_count=5, negative=15, iter=10,\n",
        "               workers=multiprocessing.cpu_count())\n",
        "\n",
        "word_vectors = w2v.wv  # get trained embeddings - an KeyedVector instaces\n",
        "result = word_vectors.similar_by_word(\"Saturday\")\n",
        "print(\"Most Similar to 'Saturday': %s\" % result[:3])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n",
            "Most Similar to 'Saturday': [('Monday', 0.8990958333015442), ('Sunday', 0.8786969780921936), ('Friday', 0.8786047697067261)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP28W_vPQjcn",
        "colab_type": "code",
        "outputId": "66856b5b-2b61-485d-e9de-cd31bc352961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from nltk.corpus import conll2000\n",
        "from gensim.models import Word2Vec # https://code.google.com/archive/p/word2vec/\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "train_words = conll2000.tagged_words('train.txt')\n",
        "test_words = conll2000.tagged_words('test.txt')\n",
        "print(train_words[:20])\n",
        "print(\"Amount of Trained Word-Tuple: %s\" % len(train_words))\n",
        "print(\"Amount of Test Word-Tuple: %s\" % len(test_words))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN')]\n",
            "Amount of Trained Word-Tuple: 211727\n",
            "Amount of Test Word-Tuple: 47377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDq9g5uyQmG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tag_vocabulary(tagged_words):\n",
        "  \"\"\"\n",
        "  Accepts text in the form of (word, pos) tuples and returns\n",
        "  a dictionary mapping POS-tags to unique ids\n",
        "  \"\"\"\n",
        "  tag2id = {}\n",
        "  for item in tagged_words:\n",
        "    tag = item[1]\n",
        "    tag2id.setdefault(tag, len(tag2id))\n",
        "  return tag2id\n",
        "\n",
        "# the word_vectors.vocab dictionary stores Vocab objects, rather than integers\n",
        "# but we would like our dictionary to map words to ints\n",
        "# the word vector is some the text, we are going to analyse\n",
        "word2id = {k: v.index for k, v in word_vectors.vocab.items()}\n",
        "# Result:  {'The': 14, 'Fulton': 5615, 'County': 1280, 'Grand': 5377, 'said': 59, 'Friday': 1852, 'an': 34, ...\n",
        "tag2id = get_tag_vocabulary(train_words) \n",
        "# Result: {'NN': 0, 'IN': 1, 'DT': 2, 'VBZ': 3, 'RB': 4, 'VBN': 5, 'TO': 6, 'VB': 7, 'JJ': 8, 'NNS': 9, 'NNP': 10, ',': 11, 'CC': 12, 'POS': 13, '.': 14, 'VBP': 15, 'VBG': 16, 'PRP$': 17, 'CD': 18, '``': 19, \"''\": 20, 'VBD': 21, 'EX': 22, 'MD': 23, '#': 24, '(': 25, '$': 26, ')': 27, 'NNPS': 28, 'PRP': 29, 'JJS': 30, 'WP': 31, 'RBR': 32, 'JJR': 33, 'WDT': 34, 'WRB': 35, 'RBS': 36, 'PDT': 37, 'RP': 38, ':': 39, 'FW': 40, 'WP$': 41, 'SYM': 42, 'UH': 43}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxe0HHoIQpTc",
        "colab_type": "code",
        "outputId": "9d647945-e791-4ca2-f819-e563bbf2f025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "UNK_INDEX = 0 # it is generally common to associate UNK with index 0\n",
        "UNK_TOKEN = \"UNK\"\n",
        "\n",
        "def get_int_data(tagged_words, word2id, tag2id):\n",
        "  \"\"\"\n",
        "  Replaces all words and tags with their corresponding ids and\n",
        "  separates words (features) from the tags (labels). \n",
        "  \"\"\"\n",
        "\n",
        "  X, Y = [], [] # X will hold word ids, Y will hold ids of their tags\n",
        "  unk_count = 0 # to keep track of the number of unkonwn words\n",
        "                # - words we don't have a representation for\n",
        "\n",
        "  for word, tag in tagged_words:\n",
        "    Y.append(tag2id.get(tag))\n",
        "    if word in word2id:\n",
        "      X.append(word2id.get(word))\n",
        "    else:\n",
        "      X.append(UNK_INDEX) # <---- NEW ADDED!\n",
        "      unk_count += 1\n",
        "  print(\"Data Created. percentag of unkown words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "  return np.array(X), np.array(Y)\n",
        "\n",
        "X_train, Y_train = get_int_data(train_words, word2id, tag2id)\n",
        "X_test, Y_test = get_int_data(test_words, word2id, tag2id)\n",
        "\n",
        "print(\"Result Data: %s, %s\" %(len(X_train), len(Y_train)))\n",
        "\n",
        "print(X_train)\n",
        "print(Y_train)\n",
        "# we need to one-hot encode the tag indexes\n",
        "Y_train, Y_test = to_categorical(Y_train), to_categorical(Y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Created. percentag of unkown words: 0.143\n",
            "Data Created. percentag of unkown words: 0.149\n",
            "Result Data: 211727, 211727\n",
            "[   0    7    0 ... 2749  801    2]\n",
            "[ 0  1  2 ... 10  4 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1tTdIFZSX8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
        "  \"\"\"\n",
        "  Adds a new word to the existing matrix of word embeddings.\n",
        "  \"\"\"\n",
        "  # inserts the vector before given index, along axis 0\n",
        "  embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
        "\n",
        "  # updating the indexes of words that follow the new word\n",
        "  word2id = {word: (index + 1) if index >= new_index else index \n",
        "             for word, index in word2id.items()}\n",
        "  word2id[new_word] = new_index\n",
        "  return embedding_matrix, word2id\n",
        "\n",
        "\n",
        "embedding_matrix = word_vectors.vectors\n",
        "unk_vector = embedding_matrix.mean(0)\n",
        "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDDrNymLTa37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "5d7afbda-afff-4f2c-8cf9-f860123515c8"
      },
      "source": [
        "HIDDEN_SIZE = 50\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def define_model(embedding_matrix, class_count):\n",
        "  \"\"\"\n",
        "  Create and returns a simple part-of-speech model, which\n",
        "  takes only one word as input\n",
        "  \"\"\"\n",
        "  vocab_length = len(embedding_matrix)\n",
        "  model = Sequential() # a sequential model is a stack of layers - we will add them one by one\n",
        "\n",
        "  # A layer which turns word indexes into vectors\n",
        "  model.add(Embedding(input_dim = vocab_length,\n",
        "                      output_dim=EMB_DIM, # output of this layer is the embedding of the input word\n",
        "                      weights=[embedding_matrix], # the matrix holding the trained embeddings\n",
        "                      input_length=1)) # specifies how many indexes we are looking up\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(HIDDEN_SIZE))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "  model.add(Dense(class_count))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "pos_model = define_model(embedding_matrix, len(tag2id))\n",
        "pos_model.summary()\n",
        "\n",
        "# Training the model\n",
        "pos_model.fit(X_train,\n",
        "              Y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              verbose=1)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1, 300)            4552200   \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                15050     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 44)                2244      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 44)                0         \n",
            "=================================================================\n",
            "Total params: 4,569,494\n",
            "Trainable params: 4,569,494\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 211727 samples\n",
            "211727/211727 [==============================] - 72s 339us/sample - loss: 0.8835 - accuracy: 0.7449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f04acf73080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYBoCILXTPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5f9bc68f-bd79-4304-9e96-e93f10c83a9c"
      },
      "source": [
        "def evaluate_model(model, id2word, x_test, y_test):\n",
        "  \"\"\"\n",
        "  Evaluates the given model by computing the accuracy of its predictions\n",
        "  on the given test data and prints out 10 most mistagged words.\n",
        "  \"\"\"\n",
        "  _, acc = model.evaluate(x_test, y_test) # get accuracy of the model\n",
        "  print(\"Accuracy: %.2f\" % acc)\n",
        "\n",
        "\n",
        "  # the following lines are used to get most commonly misstagged words\n",
        "  y_pred = model.predict_classes(x_test) # get model predictions\n",
        "  error_counter = collections.Counter()  # we will use a counter instance to count model's erros\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
        "    if y_pred[i] != correct_tag_id:\n",
        "      word = id2word[x_test[i]]\n",
        "      error_counter[word] += 1\n",
        "\n",
        "  print(\"Most commen errors:\\n\", error_counter.most_common(10))\n",
        "\n",
        "id2word = sorted(word2id, key=word2id.get)\n",
        "evaluate_model(pos_model, id2word, X_test, Y_test)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47377/47377 [==============================] - 2s 35us/sample - loss: 0.6086 - accuracy: 0.8021\n",
            "Accuracy: 0.80\n",
            "Most commen errors:\n",
            " [('UNK', 7062), ('in', 136), ('this', 51), ('it', 37), ('who', 30), ('miles', 18), ('its', 18), ('losses', 18), ('another', 18), ('importance', 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheDxR0Vi1g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EOS_INDEX = 1\n",
        "EOS_TOKEN = \"EOS\"\n",
        "\n",
        "# creating a random end-of-sequence vector\n",
        "eos_vector = np.random.standard_normal(EMB_DIM)\n",
        "embedding_matrix, word2id = add_new_word(EOS_TOKEN, eos_vector, EOS_INDEX, embedding_matrix, word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF361olajIum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = 2 # define the size of the context-window\n",
        "\n",
        "def get_window_int_data(tagged_words, word2id, tag2id):\n",
        "  \"\"\"\n",
        "  Replaces all words and tags with their corresponding ids and\n",
        "  generates an array of label ids Y and the traing data X which\n",
        "  consists of arrays of word indexes (of tagged word and its context).\n",
        "  \"\"\"\n",
        "  X, Y = [],[]\n",
        "  unk_count = 0\n",
        "\n",
        "  span = 2 * CONTEXT_SIZE + 1 # the complete span of the sliding window -> [ window traget window]\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "  padding = [(EOS_TOKEN, None)] * CONTEXT_SIZE\n",
        "  buffer += padding + tagged_words[:CONTEXT_SIZE]\n",
        "\n",
        "  for item in (tagged_words[CONTEXT_SIZE:] + padding):\n",
        "    buffer.append(item)\n",
        "\n",
        "    # the input to the model is the ids of all words in the window\n",
        "    window_ids = np.array([word2id.get(word) if (word in word2id) else UNK_INDEX for (word, _) in buffer])\n",
        "\n",
        "    X.append(window_ids)\n",
        "\n",
        "    # the label is the tag of the middle word\n",
        "    middle_word, middle_tag = buffer[CONTEXT_SIZE]\n",
        "    Y.append(tag2id.get(middle_tag))\n",
        "\n",
        "    if middle_word not in word2id:\n",
        "      unk_count += 1\n",
        "\n",
        "  print(\"Data Created, Percentage of unknown words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "  return np.array(X), np.array(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7mkIxWkkWjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_context_sensitive_model(embedding_matrix, class_count):\n",
        "  \"\"\"\n",
        "  Create and returns a part-of-speech model, which\n",
        "  takes as input a tagged word and its context.\n",
        "  \"\"\"\n",
        "\n",
        "  vocab_length = len(embedding_matrix)\n",
        "  total_span = CONTEXT_SIZE * 2 +1 \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_length,\n",
        "                       output_dim=EMB_DIM,\n",
        "                       weights=[embedding_matrix],\n",
        "                       input_length=total_span)), # <----\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(HIDDEN_SIZE))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "  model.add(Dense(class_count))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WyMyCHAlC1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_ext(model, id2word, x_test, y_test):\n",
        "  \"\"\"\n",
        "  Evaluates the given model by computing the accuracy of its predictions\n",
        "  on the given test data and prints out 10 most mistagged words.\n",
        "  \"\"\"\n",
        "  _, acc = model.evaluate(x_test, y_test) # get accuracy of the model\n",
        "  print(\"Accuracy: %.2f\" % acc)\n",
        "\n",
        "\n",
        "  # the following lines are used to get most commonly misstagged words\n",
        "  y_pred = model.predict_classes(x_test) # get model predictions\n",
        "  error_counter = collections.Counter()  # we will use a counter instance to count model's erros\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
        "    if y_new[i] != correct_tag_di:\n",
        "      if isinstance(x_test[i], np.ndarray): \n",
        "        word = id2word[x_test[i][CONTEXT_SIZE]]\n",
        "      else:\n",
        "        word = id2word[x_test[i]]\n",
        "      error_counter[word] += 1\n",
        "  \n",
        "  print(\"Most commen errors:\\n\", error_counter.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw-KUOURljDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "7c51260f-fdd7-496d-d678-1cf5217c39cf"
      },
      "source": [
        "X_train2, Y_train2 = get_window_int_data(train_words, word2id, tag2id)\n",
        "X_test2, Y_test2 = get_window_int_data(test_words, word2id, tag2id)\n",
        "Y_train2, Y_test2 = to_categorical(Y_train2), to_categorical(Y_test2)\n",
        "\n",
        "\n",
        "cs_pos_model = define_context_sensitive_model(embedding_matrix, len(tag2id))\n",
        "cs_pos_model.fit(X_train2,\n",
        "                 Y_train2,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=1,\n",
        "                 verbose=1)\n",
        "\n",
        "evaluate_model_ext(cs_pos_model, id2word, X_test2, Y_test2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Created, Percentage of unknown words: 0.143\n",
            "Data Created, Percentage of unknown words: 0.149\n",
            "Train on 211727 samples\n",
            "211727/211727 [==============================] - 76s 359us/sample - loss: 0.4861 - accuracy: 0.8652\n",
            "47377/47377 [==============================] - 2s 45us/sample - loss: 0.2879 - accuracy: 0.9126\n",
            "Accuracy: 0.91\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-26f6d5d06b31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                  verbose=1)\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mevaluate_model_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs_pos_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-6d4ffd5bcb2b>\u001b[0m in \u001b[0;36mevaluate_model_ext\u001b[0;34m(model, id2word, x_test, y_test)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mcorrect_tag_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# turn a one-hot encoding to an index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0my_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcorrect_tag_di\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONTEXT_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_new' is not defined"
          ]
        }
      ]
    }
  ]
}