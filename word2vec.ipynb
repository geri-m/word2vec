{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMevKZ63Bmykc29HJXmbz7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geri-m/word2vec/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teduj7O3P2mc",
        "colab_type": "code",
        "outputId": "bdc17470-9a41-495c-be24-bae27c289e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296\n",
        "import multiprocessing\n",
        "\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')\n",
        "\n",
        "# Data is processed and Tokenised!\n",
        "sentences = brown.sents()\n",
        "print(sentences[:3])\n",
        "\n",
        "EMB_DIM = 300\n",
        "\n",
        "w2v = Word2Vec(sentences, size=EMB_DIM, window=5, min_count=5, negative=15, iter=10,\n",
        "               workers=multiprocessing.cpu_count())\n",
        "\n",
        "word_vectors = w2v.wv  # get trained embeddings - an KeyedVector instaces\n",
        "result = word_vectors.similar_by_word(\"Saturday\")\n",
        "print(\"Most Similar to 'Saturday': %s\" % result[:3])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n",
            "Most Similar to 'Saturday': [('Monday', 0.896481990814209), ('Sunday', 0.8804537653923035), ('Friday', 0.8780795335769653)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP28W_vPQjcn",
        "colab_type": "code",
        "outputId": "5dcca30b-8fc1-42c2-df42-c0f2f70ad40d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "from nltk.corpus import conll2000\n",
        "from gensim.models import Word2Vec # https://code.google.com/archive/p/word2vec/\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "train_words = conll2000.tagged_words('train.txt')\n",
        "test_words = conll2000.tagged_words('test.txt')\n",
        "print(train_words[:20])\n",
        "print(\"Amount of Trained Word-Tuple: %s\" % len(train_words))\n",
        "print(\"Amount of Test Word-Tuple: %s\" % len(test_words))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN')]\n",
            "Amount of Trained Word-Tuple: 211727\n",
            "Amount of Test Word-Tuple: 47377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDq9g5uyQmG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tag_vocabulary(tagged_words):\n",
        "  \"\"\"\n",
        "  Accepts text in the form of (word, pos) tuples and returns\n",
        "  a dictionary mapping POS-tags to unique ids\n",
        "  \"\"\"\n",
        "  tag2id = {}\n",
        "  for item in tagged_words:\n",
        "    tag = item[1]\n",
        "    tag2id.setdefault(tag, len(tag2id))\n",
        "  return tag2id\n",
        "\n",
        "# the word_vectors.vocab dictionary stores Vocab objects, rather than integers\n",
        "# but we would like our dictionary to map words to ints\n",
        "# the word vector is some the text, we are going to analyse\n",
        "word2id = {k: v.index for k, v in word_vectors.vocab.items()}\n",
        "# Result:  {'The': 14, 'Fulton': 5615, 'County': 1280, 'Grand': 5377, 'said': 59, 'Friday': 1852, 'an': 34, ...\n",
        "tag2id = get_tag_vocabulary(train_words) \n",
        "# Result: {'NN': 0, 'IN': 1, 'DT': 2, 'VBZ': 3, 'RB': 4, 'VBN': 5, 'TO': 6, 'VB': 7, 'JJ': 8, 'NNS': 9, 'NNP': 10, ',': 11, 'CC': 12, 'POS': 13, '.': 14, 'VBP': 15, 'VBG': 16, 'PRP$': 17, 'CD': 18, '``': 19, \"''\": 20, 'VBD': 21, 'EX': 22, 'MD': 23, '#': 24, '(': 25, '$': 26, ')': 27, 'NNPS': 28, 'PRP': 29, 'JJS': 30, 'WP': 31, 'RBR': 32, 'JJR': 33, 'WDT': 34, 'WRB': 35, 'RBS': 36, 'PDT': 37, 'RP': 38, ':': 39, 'FW': 40, 'WP$': 41, 'SYM': 42, 'UH': 43}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxe0HHoIQpTc",
        "colab_type": "code",
        "outputId": "0c26aae0-47e9-4fc8-ef95-368ba8acc404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "UNK_INDEX = 0 # it is generally common to associate UNK with index 0\n",
        "UNK_TOKEN = \"UNK\"\n",
        "\n",
        "def get_int_data(tagged_words, word2id, tag2id):\n",
        "  \"\"\"\n",
        "  Replaces all words and tags with their corresponding ids and\n",
        "  separates words (features) from the tags (labels). \n",
        "  \"\"\"\n",
        "\n",
        "  X, Y = [], [] # X will hold word ids, Y will hold ids of their tags\n",
        "  unk_count = 0 # to keep track of the number of unkonwn words\n",
        "                # - words we don't have a representation for\n",
        "\n",
        "  for word, tag in tagged_words:\n",
        "    Y.append(tag2id.get(tag))\n",
        "    if word in word2id:\n",
        "      X.append(word2id.get(word))\n",
        "    else:\n",
        "      X.append(UNK_INDEX) # <---- NEW ADDED!\n",
        "      unk_count += 1\n",
        "  print(\"Data Created. percentag of unkown words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "  return np.array(X), np.array(Y)\n",
        "\n",
        "X_train, Y_train = get_int_data(train_words, word2id, tag2id)\n",
        "X_test, Y_test = get_int_data(test_words, word2id, tag2id)\n",
        "\n",
        "print(\"Result Data: %s, %s\" %(len(X_train), len(Y_train)))\n",
        "\n",
        "print(X_train)\n",
        "print(Y_train)\n",
        "# we need to one-hot encode the tag indexes\n",
        "Y_train, Y_test = to_categorical(Y_train), to_categorical(Y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Created. percentag of unkown words: 0.143\n",
            "Data Created. percentag of unkown words: 0.149\n",
            "Result Data: 211727, 211727\n",
            "[   0    7    0 ... 2749  801    2]\n",
            "[ 0  1  2 ... 10  4 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1tTdIFZSX8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
        "  \"\"\"\n",
        "  Adds a new word to the existing matrix of word embeddings.\n",
        "  \"\"\"\n",
        "  # inserts the vector before given index, along axis 0\n",
        "  embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
        "\n",
        "  # updating the indexes of words that follow the new word\n",
        "  word2id = {word: (index + 1) if index >= new_index else index \n",
        "             for word, index in word2id.items()}\n",
        "  word2id[new_word] = new_index\n",
        "  return embedding_matrix, word2id\n",
        "\n",
        "\n",
        "embedding_matrix = word_vectors.vectors\n",
        "unk_vector = embedding_matrix.mean(0)\n",
        "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDDrNymLTa37",
        "colab_type": "code",
        "outputId": "d492206f-a72b-4a68-a339-d4fe0dc3ff3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "HIDDEN_SIZE = 50\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def define_model(embedding_matrix, class_count):\n",
        "  \"\"\"\n",
        "  Create and returns a simple part-of-speech model, which\n",
        "  takes only one word as input\n",
        "  \"\"\"\n",
        "  vocab_length = len(embedding_matrix)\n",
        "  model = Sequential() # a sequential model is a stack of layers - we will add them one by one\n",
        "\n",
        "  # A layer which turns word indexes into vectors\n",
        "  model.add(Embedding(input_dim = vocab_length,\n",
        "                      output_dim=EMB_DIM, # output of this layer is the embedding of the input word\n",
        "                      weights=[embedding_matrix], # the matrix holding the trained embeddings\n",
        "                      input_length=1)) # specifies how many indexes we are looking up\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(HIDDEN_SIZE))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "  model.add(Dense(class_count))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "pos_model = define_model(embedding_matrix, len(tag2id))\n",
        "pos_model.summary()\n",
        "\n",
        "# Training the model\n",
        "pos_model.fit(X_train,\n",
        "              Y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              verbose=1)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 1, 300)            4552200   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                15050     \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 44)                2244      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 44)                0         \n",
            "=================================================================\n",
            "Total params: 4,569,494\n",
            "Trainable params: 4,569,494\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f38b2217f28> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f38b2217f28> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1655/1655 [==============================] - 71s 43ms/step - loss: 0.8823 - accuracy: 0.7437\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f38acecc668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYBoCILXTPT",
        "colab_type": "code",
        "outputId": "c5da72a5-b05a-4661-daf9-e18b491bddf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "def evaluate_model(model, id2word, x_test, y_test):\n",
        "  \"\"\"\n",
        "  Evaluates the given model by computing the accuracy of its predictions\n",
        "  on the given test data and prints out 10 most mistagged words.\n",
        "  \"\"\"\n",
        "  _, acc = model.evaluate(x_test, y_test) # get accuracy of the model\n",
        "  print(\"Accuracy: %.2f\" % acc)\n",
        "\n",
        "\n",
        "  # the following lines are used to get most commonly misstagged words\n",
        "  y_pred = model.predict_classes(x_test) # get model predictions\n",
        "  error_counter = collections.Counter()  # we will use a counter instance to count model's erros\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
        "    if y_pred[i] != correct_tag_id:\n",
        "      word = id2word[x_test[i]]\n",
        "      error_counter[word] += 1\n",
        "\n",
        "  print(\"Most commen errors:\\n\", error_counter.most_common(10))\n",
        "\n",
        "id2word = sorted(word2id, key=word2id.get)\n",
        "evaluate_model(pos_model, id2word, X_test, Y_test)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f38ace28730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f38ace28730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1481/1481 [==============================] - 2s 1ms/step - loss: 0.6100 - accuracy: 0.8027\n",
            "Accuracy: 0.80\n",
            "WARNING:tensorflow:From <ipython-input-8-b33525f2b94f>:11: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f38aa014bf8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f38aa014bf8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Most commen errors:\n",
            " [('UNK', 7062), ('in', 136), ('this', 51), ('it', 37), ('who', 30), ('She', 30), ('losses', 21), ('another', 21), ('its', 18), ('installed', 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheDxR0Vi1g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EOS_INDEX = 1\n",
        "EOS_TOKEN = \"EOS\"\n",
        "\n",
        "# creating a random end-of-sequence vector\n",
        "eos_vector = np.random.standard_normal(EMB_DIM)\n",
        "embedding_matrix, word2id = add_new_word(EOS_TOKEN, eos_vector, EOS_INDEX, embedding_matrix, word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF361olajIum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = 2 # define the size of the context-window\n",
        "\n",
        "def get_window_int_data(tagged_words, word2id, tag2id):\n",
        "  \"\"\"\n",
        "  Replaces all words and tags with their corresponding ids and\n",
        "  generates an array of label ids Y and the traing data X which\n",
        "  consists of arrays of word indexes (of tagged word and its context).\n",
        "  \"\"\"\n",
        "  X, Y = [],[]\n",
        "  unk_count = 0\n",
        "\n",
        "  span = 2 * CONTEXT_SIZE + 1 # the complete span of the sliding window -> [ window traget window]\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "  padding = [(EOS_TOKEN, None)] * CONTEXT_SIZE\n",
        "  buffer += padding + tagged_words[:CONTEXT_SIZE]\n",
        "\n",
        "  for item in (tagged_words[CONTEXT_SIZE:] + padding):\n",
        "    buffer.append(item)\n",
        "\n",
        "    # the input to the model is the ids of all words in the window\n",
        "    window_ids = np.array([word2id.get(word) if (word in word2id) else UNK_INDEX for (word, _) in buffer])\n",
        "\n",
        "    X.append(window_ids)\n",
        "\n",
        "    # the label is the tag of the middle word\n",
        "    middle_word, middle_tag = buffer[CONTEXT_SIZE]\n",
        "    Y.append(tag2id.get(middle_tag))\n",
        "\n",
        "    if middle_word not in word2id:\n",
        "      unk_count += 1\n",
        "\n",
        "  print(\"Data Created, Percentage of unknown words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "  return np.array(X), np.array(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7mkIxWkkWjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_context_sensitive_model(embedding_matrix, class_count):\n",
        "  \"\"\"\n",
        "  Create and returns a part-of-speech model, which\n",
        "  takes as input a tagged word and its context.\n",
        "  \"\"\"\n",
        "\n",
        "  vocab_length = len(embedding_matrix)\n",
        "  total_span = CONTEXT_SIZE * 2 +1 \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_length,\n",
        "                       output_dim=EMB_DIM,\n",
        "                       weights=[embedding_matrix],\n",
        "                       input_length=total_span)), # <----\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(HIDDEN_SIZE))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "  model.add(Dense(class_count))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WyMyCHAlC1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_ext(model, id2word, x_test, y_test):\n",
        "  \"\"\"\n",
        "  Evaluates the given model by computing the accuracy of its predictions\n",
        "  on the given test data and prints out 10 most mistagged words.\n",
        "  \"\"\"\n",
        "  _, acc = model.evaluate(x_test, y_test) # get accuracy of the model\n",
        "  print(\"Accuracy: %.2f\" % acc)\n",
        "\n",
        "\n",
        "  # the following lines are used to get most commonly misstagged words\n",
        "  y_pred = model.predict_classes(x_test) # get model predictions\n",
        "  error_counter = collections.Counter()  # we will use a counter instance to count model's erros\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
        "    if y_pred[i] != correct_tag_id:       # sic! y_pred <=> y_new\n",
        "      if isinstance(x_test[i], np.ndarray): \n",
        "        word = id2word[x_test[i][CONTEXT_SIZE]]\n",
        "      else:\n",
        "        word = id2word[x_test[i]]\n",
        "      error_counter[word] += 1\n",
        "  \n",
        "  print(\"Most commen errors:\\n\", error_counter.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw-KUOURljDX",
        "colab_type": "code",
        "outputId": "a2ae3ce2-fa9b-4d20-8c06-3ade9c73876b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "X_train2, Y_train2 = get_window_int_data(train_words, word2id, tag2id)\n",
        "X_test2, Y_test2 = get_window_int_data(test_words, word2id, tag2id)\n",
        "Y_train2, Y_test2 = to_categorical(Y_train2), to_categorical(Y_test2)\n",
        "\n",
        "\n",
        "cs_pos_model = define_context_sensitive_model(embedding_matrix, len(tag2id))\n",
        "cs_pos_model.fit(X_train2,\n",
        "                 Y_train2,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=1,\n",
        "                 verbose=1)\n",
        "\n",
        "evaluate_model_ext(cs_pos_model, id2word, X_test2, Y_test2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Created, Percentage of unknown words: 0.143\n",
            "Data Created, Percentage of unknown words: 0.149\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f38a958a268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f38a958a268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1655/1655 [==============================] - 75s 46ms/step - loss: 0.4877 - accuracy: 0.8655\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f38a9503730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f38a9503730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "1481/1481 [==============================] - 2s 1ms/step - loss: 0.2864 - accuracy: 0.9128\n",
            "Accuracy: 0.91\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f38a8c1bf28> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f38a8c1bf28> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: Bad argument number for Name: 4, expecting 3\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Most commen errors:\n",
            " [('UNK', 2875), ('is', 54), ('out', 25), ('so', 17), ('old', 13), ('forget', 11), ('It', 10), ('deserve', 9), ('station', 8), ('head', 7)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}