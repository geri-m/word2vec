{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOJ8lDkLMiJU3glRdYFkax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geri-m/word2vec/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49DA25L3uc9h",
        "colab_type": "text"
      },
      "source": [
        "# Goal\n",
        "\n",
        "The Goal of this sample to so tag (= classify) word types of a given text. The model should be able to answer for example the following questions as accuarte as possible. \n",
        "```\n",
        "\"Is the word 'will' in the sentense 'This is my last will' a noun?\"\n",
        "\"Is the word 'will' in the sentense 'I will do that' a verb?\"\n",
        "```\n",
        "\n",
        "## Background Information\n",
        "\n",
        "The tags will be applied according to [conll2000](https://www.clips.uantwerpen.be/conll2000/chunking/), (Conference on Computational Natural Language Learning).\n",
        "\n",
        "## Demonstration\n",
        "\n",
        "The demo will show that by adding more context to a word (= taking surounding words into consideration), it is more likly to classify the word correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxP_Kibhwfva",
        "colab_type": "text"
      },
      "source": [
        "# Prerequists\n",
        "\n",
        "At first we make sure, Tensorflow is installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZikXryawMnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow Version: %s\" % tf.__version__)\n",
        "print(\"GPUs available: %s \" % tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teduj7O3P2mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296\n",
        "import multiprocessing\n",
        "\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')\n",
        "\n",
        "# Data is processed and Tokenised!\n",
        "sentences = brown.sents()\n",
        "print(sentences[:3])\n",
        "\n",
        "EMB_DIM = 300\n",
        "\n",
        "w2v = Word2Vec(sentences, size=EMB_DIM, window=5, min_count=5, negative=15, iter=10,\n",
        "               workers=multiprocessing.cpu_count())\n",
        "\n",
        "word_vectors = w2v.wv  # get trained embeddings - an KeyedVector instaces\n",
        "result = word_vectors.similar_by_word(\"Saturday\")\n",
        "print(\"Most Similar to 'Saturday': %s\" % result[:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKNlcZYSlk4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import islice\n",
        "\n",
        "for index, item in islice(word_vectors.vocab.items(), 10):\n",
        "  print(\"Word: '%s'\\tIndex in Dict: %s\\tTotal Count in Input Text: %s\" % (index, item.index, item.count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP28W_vPQjcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import conll2000\n",
        "from gensim.models import Word2Vec # https://code.google.com/archive/p/word2vec/\n",
        "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "train_words = conll2000.tagged_words('train.txt')\n",
        "test_words = conll2000.tagged_words('test.txt')\n",
        "print(train_words[:20])\n",
        "print(test_words[:20])\n",
        "print(\"Amount of Trained Word-Tuple: %s\" % len(train_words))\n",
        "print(\"Amount of Test Word-Tuple: %s\" % len(test_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDq9g5uyQmG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Tuple\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "from typing import NoReturn\n",
        "\n",
        "def get_tag_vocabulary(tagged_words: List[Tuple[str, str]]) -> Dict[str, int]:\n",
        "  \"\"\"\n",
        "  Accepts text in the form of (word, pos) tuples and returns\n",
        "  a dictionary mapping POS-tags to unique ids\n",
        "  \"\"\"\n",
        "  tag2id = {}\n",
        "  for item in tagged_words:\n",
        "    tag = item[1]\n",
        "    tag2id.setdefault(tag, len(tag2id))\n",
        "  return tag2id\n",
        "\n",
        "# the word_vectors.vocab dictionary stores Vocab objects, rather than integers\n",
        "# but we would like our dictionary to map words to ints\n",
        "# the word vector is some text, we are going to analyse\n",
        "word2id = {k: v.index for k, v in word_vectors.vocab.items()}\n",
        "# Result:  {'The': 14, 'Fulton': 5615, 'County': 1280, 'Grand': 5377, 'said': 59, 'Friday': 1852, 'an': 34, ...\n",
        "# tag-2-id is from the trained data set. (we don't have tags yet in sentenses from word2vec)\n",
        "tag2id = get_tag_vocabulary(train_words) \n",
        "# Result: {'NN': 0, 'IN': 1, 'DT': 2, 'VBZ': 3, 'RB': 4, 'VBN': 5, 'TO': 6, 'VB': 7, 'JJ': 8, 'NNS': 9, 'NNP': 10, ',': 11, 'CC': 12, 'POS': 13, '.': 14, 'VBP': 15, 'VBG': 16, 'PRP$': 17, 'CD': 18, '``': 19, \"''\": 20, 'VBD': 21, 'EX': 22, 'MD': 23, '#': 24, '(': 25, '$': 26, ')': 27, 'NNPS': 28, 'PRP': 29, 'JJS': 30, 'WP': 31, 'RBR': 32, 'JJR': 33, 'WDT': 34, 'WRB': 35, 'RBS': 36, 'PDT': 37, 'RP': 38, ':': 39, 'FW': 40, 'WP$': 41, 'SYM': 42, 'UH': 43}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxe0HHoIQpTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNK_INDEX = 0 # it is generally common to associate UNK with index 0\n",
        "UNK_TOKEN = \"UNK\"\n",
        "\n",
        "def get_int_data(tagged_words:List[str], word2id: Dict[str, int], tag2id:Dict[str, int]) -> Tuple[List[int], List[int]]:\n",
        "  \"\"\"\n",
        "  Replaces all words and tags with their corresponding ids and\n",
        "  separates words (features) from the tags (labels). \n",
        "  \"\"\"\n",
        "\n",
        "  X, Y = [], [] # X will hold word ids, Y will hold ids of their tags\n",
        "  unk_count = 0 # to keep track of the number of unkonwn words\n",
        "                # - words we don't have a representation for\n",
        "\n",
        "  # iterate over the list of tagged words \n",
        "  for word, tag in tagged_words:\n",
        "    # all possible tags for these tagged_words are already in tag2id\n",
        "    # so get the ID an put it into Y\n",
        "    Y.append(tag2id.get(tag))\n",
        "    # check if the corresponding word is in the data we want to analyse\n",
        "    if word in word2id:\n",
        "      # if yes add to X (only the INDEX!)\n",
        "      X.append(word2id.get(word))\n",
        "    else:\n",
        "      # if not, create an unkonwn word. (we only put INDEXs there)\n",
        "      X.append(UNK_INDEX) # <---- NEW ADDED!\n",
        "      unk_count += 1\n",
        "  print(\"Data Created. percentag of unkown words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "  return np.array(X), np.array(Y)\n",
        "\n",
        "X_train, Y_train = get_int_data(train_words, word2id, tag2id)\n",
        "X_test, Y_test = get_int_data(test_words, word2id, tag2id)\n",
        "\n",
        "print(\"Result Data Len; X: %s, Y: %s\" %(len(X_train), len(Y_train)))\n",
        "\n",
        "# we need to one-hot encode the tag indexes\n",
        "Y_train, Y_test = to_categorical(Y_train), to_categorical(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1tTdIFZSX8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_new_word(new_word: str, new_vector, new_index: int, embedding_matrix: np.ndarray, word2id: Dict[str, int]) -> Tuple[np.ndarray, List[Tuple[str, int]]]:\n",
        "  \"\"\"\n",
        "  Adds a new word to the existing matrix of word embeddings.\n",
        "  \"\"\"\n",
        "  # inserts the vector before given index, along axis 0\n",
        "  embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
        "\n",
        "  # updating the indexes of words that follow the new word\n",
        "  word2id = {word: (index + 1) if index >= new_index else index \n",
        "             for word, index in word2id.items()}\n",
        "  word2id[new_word] = new_index\n",
        "  return embedding_matrix, word2id\n",
        "\n",
        "# we add one single vector for the Unknown words\n",
        "embedding_matrix = word_vectors.vectors\n",
        "unk_vector = embedding_matrix.mean(0)\n",
        "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDDrNymLTa37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 50\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def define_model(embedding_matrix: np.ndarray, class_count: int) -> Sequential:\n",
        "  \"\"\"\n",
        "  Create and returns a simple part-of-speech model, which\n",
        "  takes only one word as input\n",
        "  \"\"\"\n",
        "  vocab_length = len(embedding_matrix)\n",
        "  model = Sequential() # a sequential model is a stack of layers - we will add them one by one\n",
        "\n",
        "  # Create and composing the model is the secret ingredint to Machine Learning\n",
        "  # A layer which turns word indexes into vectors\n",
        "  model.add(Embedding(input_dim = vocab_length,\n",
        "                      output_dim=EMB_DIM, # output of this layer is the embedding of the input word\n",
        "                      weights=[embedding_matrix], # the matrix holding the trained embeddings\n",
        "                      input_length=1)) # specifies how many indexes we are looking up\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(HIDDEN_SIZE))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "  model.add(Dense(class_count))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "pos_model = define_model(embedding_matrix, len(tag2id))\n",
        "pos_model.summary()\n",
        "\n",
        "# Training the model. \n",
        "# Input Parameter for the model are words of a tokenzied senetense (Indexes only)\n",
        "# Output Parameter of the model are types/tags of the words\n",
        "pos_model.fit(X_train,\n",
        "              Y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              verbose=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KYBoCILXTPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model : Sequential, id2word:List[str], x_test: List[int], y_test: List[int]):\n",
        "  \"\"\"\n",
        "  Evaluates the given model by computing the accuracy of its predictions\n",
        "  on the given test data and prints out 10 most mistagged words.\n",
        "  \"\"\"\n",
        "  _, acc = model.evaluate(x_test, y_test) # get accuracy of the model\n",
        "  print(\"Accuracy: %.2f\" % acc)\n",
        "\n",
        "\n",
        "  # the following lines are used to get most commonly misstagged words\n",
        "  y_pred = model.predict_classes(x_test) # get model predictions\n",
        "  error_counter = collections.Counter()  # we will use a counter instance to count model's erros\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
        "    if y_pred[i] != correct_tag_id:\n",
        "      word = id2word[x_test[i]]\n",
        "      error_counter[word] += 1\n",
        "\n",
        "  print(\"Most commen errors:\\n\", error_counter.most_common(10))\n",
        "\n",
        "# List of all words sorted by their ID\n",
        "id2word = sorted(word2id, key=word2id.get)\n",
        "evaluate_model(pos_model, id2word, X_test, Y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WheDxR0Vi1g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EOS_INDEX = 1\n",
        "EOS_TOKEN = \"EOS\"\n",
        "\n",
        "# creating a random end-of-sequence vector\n",
        "eos_vector = np.random.standard_normal(EMB_DIM)\n",
        "embedding_matrix, word2id = add_new_word(EOS_TOKEN, eos_vector, EOS_INDEX, embedding_matrix, word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF361olajIum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = 2 # define the size of the context-window\n",
        "\n",
        "\n",
        "def get_window_int_data(tagged_words: List[Tupel[str, str]], word2id: Dict[str, int], tag2id: Dict[str, int]) -> Tuple[List[List[int]]]:\n",
        "  \"\"\"\n",
        "  Replaces all words and tags with their corresponding ids and\n",
        "  generates an array of label ids Y and the traing data X which\n",
        "  consists of arrays of word indexes (of tagged word and its context).\n",
        "  \"\"\"\n",
        "\n",
        "  # create list-of-list\n",
        "  X, Y = [],[]\n",
        "  unk_count = 0\n",
        "\n",
        "  span = 2 * CONTEXT_SIZE + 1 # the complete span of the sliding window -> [ window traget window]\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "  padding = [(EOS_TOKEN, None)] * CONTEXT_SIZE\n",
        "  buffer += padding + tagged_words[:CONTEXT_SIZE]\n",
        "\n",
        "  for item in (tagged_words[CONTEXT_SIZE:] + padding):\n",
        "    buffer.append(item)\n",
        "\n",
        "    # the input to the model is the ids of all words in the window\n",
        "    window_ids = np.array([word2id.get(word) if (word in word2id) else UNK_INDEX for (word, _) in buffer])\n",
        "    X.append(window_ids)\n",
        "\n",
        "    # the label is the tag of the middle word\n",
        "    middle_word, middle_tag = buffer[CONTEXT_SIZE]\n",
        "    Y.append(tag2id.get(middle_tag))\n",
        "\n",
        "    if middle_word not in word2id:\n",
        "      unk_count += 1\n",
        "\n",
        "  print(\"Data Created, Percentage of unknown words: %.3f\" % (unk_count/len(tagged_words)))\n",
        "  return np.array(X), np.array(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7mkIxWkkWjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_context_sensitive_model(embedding_matrix: np.ndarray, class_count: int):\n",
        "  \"\"\"\n",
        "  Create and returns a part-of-speech model, which\n",
        "  takes as input a tagged word and its context.\n",
        "  \"\"\"\n",
        "\n",
        "  vocab_length = len(embedding_matrix)\n",
        "  total_span = CONTEXT_SIZE * 2 +1 \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_length,\n",
        "                       output_dim=EMB_DIM,\n",
        "                       weights=[embedding_matrix],\n",
        "                       input_length=total_span)), # <----\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(HIDDEN_SIZE))\n",
        "  model.add(Activation(\"tanh\"))\n",
        "  model.add(Dense(class_count))\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WyMyCHAlC1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_ext(model: Sequential, id2word:List[int], x_test:List[int], y_test:List[int]):\n",
        "  \"\"\"\n",
        "  Evaluates the given model by computing the accuracy of its predictions\n",
        "  on the given test data and prints out 10 most mistagged words.\n",
        "  \"\"\"\n",
        "  _, acc = model.evaluate(x_test, y_test) # get accuracy of the model\n",
        "  print(\"Accuracy: %.2f\" % acc)\n",
        "\n",
        "\n",
        "  # the following lines are used to get most commonly misstagged words\n",
        "  y_pred = model.predict_classes(x_test) # get model predictions\n",
        "  error_counter = collections.Counter()  # we will use a counter instance to count model's erros\n",
        "\n",
        "  for i in range(len(x_test)):\n",
        "    correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
        "    if y_pred[i] != correct_tag_id:       # sic! y_pred <=> y_new\n",
        "      if isinstance(x_test[i], np.ndarray): \n",
        "        word = id2word[x_test[i][CONTEXT_SIZE]]\n",
        "      else:\n",
        "        word = id2word[x_test[i]]\n",
        "      error_counter[word] += 1\n",
        "  \n",
        "  print(\"Most commen errors:\\n\", error_counter.most_common(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw-KUOURljDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train2, Y_train2 = get_window_int_data(train_words, word2id, tag2id)\n",
        "X_test2, Y_test2 = get_window_int_data(test_words, word2id, tag2id)\n",
        "Y_train2, Y_test2 = to_categorical(Y_train2), to_categorical(Y_test2)\n",
        "\n",
        "cs_pos_model = define_context_sensitive_model(embedding_matrix, len(tag2id))\n",
        "cs_pos_model.fit(X_train2,\n",
        "                 Y_train2,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=1,\n",
        "                 verbose=1)\n",
        "\n",
        "evaluate_model_ext(cs_pos_model, id2word, X_test2, Y_test2)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}